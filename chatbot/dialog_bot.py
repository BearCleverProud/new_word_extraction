from transformers import AutoTokenizer, AutoModel
import torch
import argparse
import json
import sys
from ltp import LTP
from elasticsearch import Elasticsearch

es = Elasticsearch()
stopwords = set()
ltp = LTP()
with open(args.stop_words, 'r') as f:
	for line in f:
		stopwords.add(line.strip())

dsl = {
    'query': {
        'match': {
            'name': ''
        }
    }
}

def ChatBot():
	def __init__(self, json_file, model_file):
		self.model_file = model_file
		self.faq = read_json("json_file")
		self.name_embeddings = torch.load(self.model_file)

	def get_res(self, msg):
		seg, hidden = ltp.seg([msg])
		seg = seg[0]
		tok = "".join(remove_stop(seg, stopwords))
		r = search(tok, faq)
		s = ""

		if r:
			s += "\n"
			s += "ä½ è¯´çš„æ˜¯" + r['name'] + "å—ï¼Ÿ"
			if 'abs' in r:
				s += r["abs"]
			if "infobox" in r:
				s += infobox_to_string(r['infobox'])
			s += "\n"
			return s

		else:
			result = esearch(tok)
			tmp = tokenizer(tok)
			sent_embed = model(torch.tensor(tmp['input_ids']).unsqueeze(0), \
				torch.tensor(tmp['attention_mask']).unsqueeze(0))[0].squeeze(0).max(dim=0)[0]
			sim = torch.tensor([cos_sim(each, sent_embed) for each in self.name_embeddings])
			best_match = torch.argsort(sim, descending=True)[:2]
			best_match_name = [faq[each]['name'].lower() for each in best_match]
			not_found = True
			for idx, each in enumerate(best_match_name):
				if each in result:
					not_found = False
					return return_s(best_match[idx].item(), faq)
			if not_found:
				return return_s(best_match[0].item(), faq)



		inp = input("ä½ å¥½å‘€æˆ‘æ˜¯ç¬¨ç¬¨ï¼Œæˆ‘çŸ¥é“æ‰€æœ‰å…³äºè®¡ç®—æœºçš„è¯è¯­å“¦ï¼Œæœ‰ä»€ä¹ˆé—®é¢˜éƒ½å¯ä»¥é—®æˆ‘å“¦ï¼ï½\n")
		seg, hidden = ltp.seg([inp])
		seg = seg[0]
		tok = "".join(remove_stop(seg, stopwords))
		print("æ‹œæ‹œğŸ‘‹ï¼Œæƒ³æˆ‘äº†å°±æ¥æ‰¾æˆ‘å“¦ï½")





tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")  
model = AutoModel.from_pretrained("bert-base-chinese")

def infobox_to_string(infobox):
	string = "\n"
	for each in infobox:
		string += each["key"] + "\t" + each["value"] + "\n"
	return string

def cos_sim(v1, v2):
	return v1.dot(v2) / torch.norm(v1) / torch.norm(v2)

def read_json(filename):
	faq = []
	with open(filename, 'r') as f:
		for line in f:
			data = json.loads(line.lower().strip())
			faq.append(data)
			# print(data)
			# es.index(index='baidubaike', body=data)
	return faq

def calculate_embedding(filename):
	name_embeddings = []
	faq = read_json(filename)
	tokenized = [tokenizer(each['name']) for each in faq]
	i = 0
	for each in tokenized:
		result = model(torch.tensor(each['input_ids']).unsqueeze(0), \
		torch.tensor(each['attention_mask']).unsqueeze(0))[0].squeeze(0).max(dim=0)
		name_embeddings.append(result[0])
		i += 1
		if i % 100 == 0:
			print(str(i) + " out of " + str(len(tokenized)) + " have been calculated")
	torch.save(name_embeddings, 'name_embeddings.pt')
	return name_embeddings

def remove_stop(tok, stopwords):
	to_be_deleted = set()
	for each in tok:
		if each in stopwords:
			to_be_deleted.add(each)
	s = []
	for each in tok:
		if each not in to_be_deleted:
			s.append(each)
	return s

def search(query, faq):
	for each in faq:
		if query == each['name']:
			return each
	return None

def esearch(s):
	dsl['query']['match']['name'] = s
	results = es.search(body=dsl)['hits']['hits']
	return [each['_source']['name'] for each in results]

def return_s(i, faq):
	s = ""
	if "abs" in faq[i]:
		s += "\n"
		s += "ä½ è¯´çš„æ˜¯" + faq[i]['name'] + "å—ï¼Ÿ"
		s += faq[i]["abs"]
		if "infobox" in faq[i]:
			s += infobox_to_string(faq[i]['infobox'])
		s += "\n"
	else:
		s += "\n"
		s += "ä½ è¯´çš„æ˜¯" + faq[i]['name'] + "å—ï¼Ÿ"
		s += "ç¬¨ç¬¨ä¹Ÿä¸çŸ¥é“å‘¢ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚è¦ä¸ä½ è‡ªå·±æŸ¥ä¸€ä¸‹ï¼Ÿï¼Ÿ"
		s += "\n"
	return s

def print_info(i, faq):
	if "abs" in faq[i]:
		print()
		print("ä½ è¯´çš„æ˜¯" + faq[i]['name'] + "å—ï¼Ÿ")
		print(faq[i]["abs"])
		if "infobox" in faq[i]:
			print(infobox_to_string(faq[i]['infobox']))
		print()
	else:
		print()
		print("ä½ è¯´çš„æ˜¯" + faq[i]['name'] + "å—ï¼Ÿ")
		print("ç¬¨ç¬¨ä¹Ÿä¸çŸ¥é“å‘¢ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚è¦ä¸ä½ è‡ªå·±æŸ¥ä¸€ä¸‹ï¼Ÿï¼Ÿ")
		print()

if __name__ == '__main__':

	parser = argparse.ArgumentParser('Chatbot System')
	parser.add_argument(
        '--gpu',
        type=int,
        default=None,
        help='gpu to run on')
	parser.add_argument(
        '--model_file',
        type=str,
        default=None,
        help='all the preprocessed queries')
	parser.add_argument(
        '--json_file',
        type=str,
        default=None,
        help='crawled from the web')
	args = parser.parse_args()
	parser.add_argument(
        '--stop_words',
        type=str,
        default='stopwords/hit_stopwords.txt',
        help='file containing the stop words')
	args = parser.parse_args()

	inp = input("ä½ å¥½å‘€æˆ‘æ˜¯ç¬¨ç¬¨ï¼Œæˆ‘çŸ¥é“æ‰€æœ‰å…³äºè®¡ç®—æœºçš„è¯è¯­å“¦ï¼Œæœ‰ä»€ä¹ˆé—®é¢˜éƒ½å¯ä»¥é—®æˆ‘å“¦ï¼ï½\n").lower()
	seg, hidden = ltp.seg([inp])
	seg = seg[0]
	tok = "".join(remove_stop(seg, stopwords))
	name_embeddings = torch.load(args.model_file) if args.model_file else calculate_embedding(args.json_file)

	while inp != "exit":

		r = search(tok, faq)

		if r:
			print()
			print("ä½ è¯´çš„æ˜¯" + r['name'] + "å—ï¼Ÿ")
			if 'abs' in r:
				print(r["abs"])
			if "infobox" in r:
				print(infobox_to_string(r['infobox']))
			print()

		else:
			result = esearch(tok)
			tmp = tokenizer(tok)
			sent_embed = model(torch.tensor(tmp['input_ids']).unsqueeze(0), \
				torch.tensor(tmp['attention_mask']).unsqueeze(0))[0].squeeze(0).max(dim=0)[0]
			sim = torch.tensor([cos_sim(each, sent_embed) for each in name_embeddings])
			best_match = torch.argsort(sim, descending=True)[:2]
			best_match_name = [faq[each]['name'].lower() for each in best_match]
			not_found = True
			for idx, each in enumerate(best_match_name):
				if each in result:
					not_found = False
					print_info(best_match[idx].item(), faq)
					break
			if not_found:
				print_info(best_match[0].item(), faq)


		inp = input("ä½ å¥½å‘€æˆ‘æ˜¯ç¬¨ç¬¨ï¼Œæˆ‘çŸ¥é“æ‰€æœ‰å…³äºè®¡ç®—æœºçš„è¯è¯­å“¦ï¼Œæœ‰ä»€ä¹ˆé—®é¢˜éƒ½å¯ä»¥é—®æˆ‘å“¦ï¼ï½\n")
		seg, hidden = ltp.seg([inp])
		seg = seg[0]
		tok = "".join(remove_stop(seg, stopwords))
	print("æ‹œæ‹œğŸ‘‹ï¼Œæƒ³æˆ‘äº†å°±æ¥æ‰¾æˆ‘å“¦ï½")





